name: AI Agent Pipeline

on:
  pull_request:
    types: [opened, synchronize, reopened]
  workflow_dispatch:
    inputs:
      pr_number:
        description: 'PR number to analyze'
        required: true
        type: number
      agents:
        description: 'Agents to run (comma-separated: validator,remediator,guardian,verifier or "all")'
        required: false
        default: 'all'
        type: string

permissions:
  pull-requests: write
  contents: read
  issues: write

jobs:
  # Job 1: Collect target PRs
  collect-prs:
    name: Collect Target PRs
    runs-on: ubuntu-latest
    outputs:
      pr_numbers: ${{ steps.get-prs.outputs.pr_numbers }}
    steps:
      - name: Get PR numbers
        id: get-prs
        run: |
          if [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
            echo "pr_numbers=[\"${{ inputs.pr_number }}\"]" >> $GITHUB_OUTPUT
          else
            echo "pr_numbers=[\"${{ github.event.pull_request.number }}\"]" >> $GITHUB_OUTPUT
          fi
      
      - name: Show target PRs
        run: |
          echo "Target PRs: ${{ steps.get-prs.outputs.pr_numbers }}"

  # Job 2: Run agents in parallel using matrix strategy
  run-agents:
    name: ${{ matrix.agent }} (PR ${{ matrix.pr }})
    runs-on: ubuntu-latest
    needs: collect-prs
    if: needs.collect-prs.outputs.pr_numbers != '[]'
    
    strategy:
      fail-fast: false
      matrix:
        pr: ${{ fromJson(needs.collect-prs.outputs.pr_numbers) }}
        agent:
          - validator
          - remediator
          - guardian
          - verifier
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pyyaml httpx
          # Install project dependencies if needed
          if [ -f pyproject.toml ]; then
            pip install -e .
          fi
      
      - name: Run ${{ matrix.agent }} agent
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          # Azure OpenAI (for validator)
          AZURE_OPENAI_API_KEY: ${{ secrets.AZURE_OPENAI_API_KEY }}
          # GitHub Models (for remediator)
          GITHUB_MODELS_TOKEN: ${{ secrets.GITHUB_MODELS_TOKEN }}
          # Moonshot (for guardian)
          MOONSHOT_API_KEY: ${{ secrets.MOONSHOT_API_KEY }}
          # OpenRouter (for verifier)
          OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
        run: |
          python -m scripts.agents.run_agent \
            --pr ${{ matrix.pr }} \
            --agent ${{ matrix.agent }} \
            --output results/${{ matrix.agent }}_pr${{ matrix.pr }}.json
        continue-on-error: true
      
      - name: Upload results
        uses: actions/upload-artifact@v4.4.3
        if: always()
        with:
          name: ${{ matrix.agent }}-pr${{ matrix.pr }}-results
          path: results/${{ matrix.agent }}_pr${{ matrix.pr }}.json
          retention-days: 30

  # Job 3: Generate summary report
  generate-summary:
    name: Generate Summary Report
    runs-on: ubuntu-latest
    needs: [collect-prs, run-agents]
    if: always() && needs.collect-prs.outputs.pr_numbers != '[]'
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Download all results
        uses: actions/download-artifact@v4.1.3
        with:
          path: results/
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
      
      - name: Generate summary
        run: |
          python << 'PYTHON_SCRIPT'
          import json
          import os
          from pathlib import Path
          
          # Find all result files
          results_dir = Path("results")
          all_results = {}
          
          for artifact_dir in results_dir.iterdir():
              if artifact_dir.is_dir():
                  for result_file in artifact_dir.glob("*.json"):
                      with open(result_file) as f:
                          data = json.load(f)
                          agent_name = result_file.stem.split("_")[0]
                          pr_num = result_file.stem.split("pr")[1].replace(".json", "")
                          
                          if pr_num not in all_results:
                              all_results[pr_num] = {}
                          all_results[pr_num][agent_name] = data
          
          # Generate markdown summary
          summary = "# ðŸ¤– AI Agent Pipeline Results\n\n"
          
          for pr_num, agents in sorted(all_results.items()):
              summary += f"## PR #{pr_num}\n\n"
              summary += "| Agent | Verdict | Summary |\n"
              summary += "|-------|---------|----------|\n"
              
              for agent_name, result in sorted(agents.items()):
                  verdict = result.get("verdict", "UNKNOWN")
                  emoji = {"PASS": "âœ…", "WARN": "âš ï¸", "FAIL": "âŒ", "BLOCK": "ðŸš«"}.get(verdict, "â“")
                  agent_display = agent_name.title()
                  summary_text = result.get("summary", "No summary")[:100]
                  
                  summary += f"| {emoji} {agent_display} | {verdict} | {summary_text} |\n"
              
              summary += "\n"
          
          # Write to GitHub Step Summary
          with open(os.environ["GITHUB_STEP_SUMMARY"], "w") as f:
              f.write(summary)
          
          print(summary)
          PYTHON_SCRIPT
      
      - name: Upload combined results
        uses: actions/upload-artifact@v4.4.3
        with:
          name: ai-agent-pipeline-results
          path: results/
          retention-days: 90

  # Job 4: Check overall status
  check-status:
    name: Check Overall Status
    runs-on: ubuntu-latest
    needs: [run-agents]
    if: always()
    
    steps:
      - name: Download all results
        uses: actions/download-artifact@v4.1.3
        with:
          path: results/
      
      - name: Check for blocking issues
        run: |
          python << 'PYTHON_SCRIPT'
          import json
          import sys
          from pathlib import Path
          
          # Find all result files
          results_dir = Path("results")
          has_block = False
          has_fail = False
          has_error = False
          
          for artifact_dir in results_dir.iterdir():
              if artifact_dir.is_dir():
                  for result_file in artifact_dir.glob("*.json"):
                      with open(result_file) as f:
                          try:
                              data = json.load(f)
                              verdict = data.get("verdict", "")
                              
                              if verdict == "BLOCK":
                                  has_block = True
                                  print(f"ðŸš« BLOCK found in {result_file.name}")
                              elif verdict == "FAIL":
                                  has_fail = True
                                  print(f"âŒ FAIL found in {result_file.name}")
                              elif "error" in data:
                                  has_error = True
                                  print(f"âš ï¸ ERROR found in {result_file.name}")
                          except json.JSONDecodeError:
                              print(f"âš ï¸ Could not parse {result_file.name}")
          
          if has_block:
              print("\nðŸš« BLOCKING ISSUES FOUND - PR should not be merged")
              sys.exit(2)
          elif has_fail or has_error:
              print("\nâŒ FAILURES FOUND - Review required")
              sys.exit(1)
          else:
              print("\nâœ… All agents passed")
              sys.exit(0)
          PYTHON_SCRIPT
